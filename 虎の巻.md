# 機械学習 虎の巻


## データ前処理

###  学習用データと教師データの分割
```python:sample
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=1)
```
### one-hot 表現
- 機械学習の教師データのラベルをベクトル化する処理。
- 通常は1,2,3のように連番でつけるがone hot 表現なら(1,0,0), (0,1,0), (0,0,1)
- 注意として上の例だと2つの特徴量が確定すればあと1つの特徴量は確定するわけである。よって、1つ目の特徴量を抜くという手法もある。
　
```python:sample
from keras.utils import np_utils
y_train = np_utils.to_categorical(y_train, num_classes)
y_test = np_utils.to_categorical(y_test, num_classes)
```
```python:sample
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
X = df[['color', 'size', 'price']].values

color_le = LabelEncoder()
X[:, 0] = color_le.fit_transform(X[:, 0])
X
ohe = OneHotEncoder(categorical_features=[0])
ohe.fit_transform(X).toarray()
```
たぶんこのpandasのやつが一番便利。文字列の列に対してone hot 変換が行われる。
```python:sample
pd.get_dummies(df[['price', 'color', 'size']])#普通のonehot
pd.get_dummies(df[['price', 'color', 'size']], drop_first=True)#特徴量が削減される
```

### min max 正規化
とりあえず０～１の値にする。
```python:sample
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
```

### 標準化
値から平均を引き標準偏差で割ることで精度を上げる。使うときはテストデータにも同様の処理を行う必要があることに注意。平均が１、標準偏差が１になる
```python:sample
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
```

### 欠損値補間
欠損値をmean(平均値)、median(中央値)、most_frequent(最頻値)で置換することができる。

```python:sample
from sklearn.preprocessing import Imputer
imr = Imputer(missing_values='NaN', strategy='mean', axis=0)
imr = imr.fit(df.values)
imputed_data = imr.transform(df.values)
imputed_data
```

### クラスラベルのエンコーディング
名義特徴量に整数値を割り振り学習で使えるデータにする。
``` python:sample
from sklearn.preprocessing import LabelEncoder

# Label encoding with sklearn's LabelEncoder
class_le = LabelEncoder()
y = class_le.fit_transform(df['classlabel'].values)
```

### 主成分分析
特徴量を減らすことで、次元の呪いを回避し、過学習を防ぐ。教師なしです。
```python:sample
from sklearn.linear_model import LogisticRegression

pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train_std)
X_test_pca = pca.transform(X_test_std)

lr = LogisticRegression()
lr = lr.fit(X_train_pca, y_train)
```


## 学習
- 目的関数  
すべての目標値から予測値を引いた最小二乗法
```math  
J(w) = \frac{1}{2}\\sum_(i)(目標値-予測値)^2
```
- バッチ勾配降下法  
目的関数を最小化するための考え方。目的関数はすべてのデータから算出されるので時間計算量が膨大。(iは全データ,jは特定の説明変数を指す)

```math
w = w + \Delta w
wj = wj + \eta J(w) * xj 
```

- 確率的勾配降下法  
データを適当な大きさに分割してそれぞれに対してバッチ勾配降下法を行う

- パーセプトロン  
重み行列と、説明変数行列を掛け合わせてそれを閾値関数に入れ予測値を算出する。  
二値分類問題しかできない。OvRで対応。
- ADALINE   
パーセプトロンで用いた閾値関数の代わりにy=xの活性化関数を用いる。
- ロジスティック回帰  
ADALINEで用いた活性化関数の代わりにシグモイド関数を用いる。（これ一般的）

- OvR  
パーセプトロンなどでは2クラス分類しかできない。よって多クラス分類を行うときは1つのクラスとその他のクラスを比較する処理をすべてのクラスに対して行っている。

-


- 正則化（L1正則化・L2正則化）  
過学習を防ぐ手法。過学習は各重みが極端な値をとることで未知のデータに対して学習がうまくいないことである。これに対して正則化では目的関数に重みの絶対値の合計を加え、重みが大きくなりすぎるとペナルティを貸す手法である。（目的関数が小さくなるように学習が行われるため。）  
正則化パラメータλで目的関数と重みのバランスを調整することができる。（実際の学習では目的関数側に逆正規化パラメータCをかける。Cが１に近ければ目的関数の最小化が優先される。0に近ければ重みの小ささが優先され、重みの合計は小さい値となる。）
```math
J(w) = C*J(w)+\lamda||w||^2
```


- softmax関数：　複数の入力値の合計を1にする。出力層で用いられる




## numpy
### append
2次元を普通にappendすると1次元に勝手に変換される。
```python:sample
print(np.append(a_2d, a_2d_ex, axis=0))
```

## matplotlib
### ヒストグラム
binsでデータの横軸を何分割で表示するかが決定される。
```python:sample
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
ax.hist(y_array, bins=6)
```

## pandas(dataframe)
### 基本メソッド
- .iloc[a,b] index a columns bの値を返す。スライスで範囲指定
- .tail() 最後の部分のデータだげ取得

## 便利メソッド
###  history plot メソッド
kerasのhistory 投げたらいい感じにプロットしてくれる。
```python:sample
def  compare_TV(history):
	import matplotlib.pyplot as plt
	acc = history.history['acc']
	val_acc = history.history['val_acc']
	loss = history.history['loss']
	val_loss = history.history['val_loss']
	epochs = range(len(acc))
	plt.plot(epochs, acc, 'bo' ,label = 'training acc')
	plt.plot(epochs, val_acc, 'b' , label= 'validation acc')
	plt.title('Training and Validation acc')
	plt.legend()
	plt.figure()
	plt.plot(epochs, loss, 'bo' ,label = 'training loss')
	plt.plot(epochs, val_loss, 'b' , label= 'validation loss')
	plt.title('Training and Validation loss')
	plt.legend()
	plt.show()
```
